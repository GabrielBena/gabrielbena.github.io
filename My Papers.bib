
@article{ghosh_nonlinear_2024,
	title = {Nonlinear fusion is optimal for a wide class of multisensory tasks},
	volume = {20},
	rights = {Creative Commons Attribution-{NonCommercial}-{NoDerivatives} 4.0 International Licence ({CC}-{BY}-{NC}-{ND})},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012246},
	doi = {10.1371/journal.pcbi.1012246},
	abstract = {Animals continuously detect information via multiple sensory channels, like vision and hearing, and integrate these signals to realise faster and more accurate decisions; a fundamental neural computation known as multisensory integration. A widespread view of this process is that multimodal neurons linearly fuse information across sensory channels. However, does linear fusion generalise beyond the classical tasks used to explore multisensory integration? Here, we develop novel multisensory tasks, which focus on the underlying statistical relationships between channels, and deploy models at three levels of abstraction: from probabilistic ideal observers to artificial and spiking neural networks. Using these models, we demonstrate that when the information provided by different channels is not independent, linear fusion performs sub-optimally and even fails in extreme cases. This leads us to propose a simple nonlinear algorithm for multisensory integration which is compatible with our current knowledge of multimodal circuits, excels in naturalistic settings and is optimal for a wide class of multisensory tasks. Thus, our work emphasises the role of nonlinear fusion in multisensory integration, and provides testable hypotheses for the field to explore at multiple levels: from single neurons to behaviour.},
	pages = {e1012246},
	number = {7},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Ghosh, Marcus and Béna, Gabriel and Bormuth, Volker and Goodman, Dan F. M.},
	urldate = {2024-09-10},
	date = {2024-07-05},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Artificial neural networks, Behavior, Neural networks, Neurons, Predation, Sensory perception, Single neuron function},
	file = {ghosh_2024_nonlinear fusion is optimal for a wide class of multisensory tasks.pdf:C\:\\Users\\Gabriel\\OneDrive - Imperial College London\\Zotero_All\\Ghosh\\ghosh_2024_nonlinear fusion is optimal for a wide class of multisensory tasks.pdf:application/pdf},
}

@misc{bena_event-based_2025,
	title = {Event-based backpropagation on the neuromorphic platform {SpiNNaker}2},
	rights = {All rights reserved},
	url = {http://arxiv.org/abs/2412.15021},
	doi = {10.48550/arXiv.2412.15021},
	abstract = {Neuromorphic computing aims to replicate the brain's capabilities for energy efficient and parallel information processing, promising a solution to the increasing demand for faster and more efficient computational systems. Efficient training of neural networks on neuromorphic hardware requires the development of training algorithms that retain the sparsity of spike-based communication during training. Here, we report on the first implementation of event-based backpropagation on the {SpiNNaker}2 neuromorphic hardware platform. We use {EventProp}, an algorithm for event-based backpropagation in spiking neural networks ({SNNs}), to compute exact gradients using sparse communication of error signals between neurons. Our implementation computes multi-layer networks of leaky integrate-and-fire neurons using discretized versions of the differential equations and their adjoints, and uses event packets to transmit spikes and error signals between network layers. We demonstrate a proof-of-concept of batch-parallelized, on-chip training of {SNNs} using the Yin Yang dataset, and provide an off-chip implementation for efficient prototyping, hyper-parameter search, and hybrid training methods.},
	number = {{arXiv}:2412.15021},
	publisher = {{arXiv}},
	author = {Béna, Gabriel and Wunderlich, Timo and Akl, Mahmoud and Vogginger, Bernhard and Mayr, Christian and Gonzalez, Hector Andres},
	urldate = {2025-03-22},
	date = {2025-03-19},
	eprinttype = {arxiv},
	eprint = {2412.15021 [cs]},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Hardware Architecture, Computer Science - Neural and Evolutionary Computing},
	file = {béna_2025_event-based backpropagation on the neuromorphic platform spinnaker2.pdf:C\:\\Users\\Gabriel\\OneDrive - Imperial College London\\Zotero_All\\Béna\\béna_2025_event-based backpropagation on the neuromorphic platform spinnaker2.pdf:application/pdf},
}

@inproceedings{andrei_deep-_2024,
	title = {Deep- Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware},
	rights = {All rights reserved},
	url = {https://ieeexplore.ieee.org/abstract/document/10942794},
	doi = {10.1109/IEEECONF60004.2024.10942794},
	abstract = {This paper explores the potential of conversion-based neuromorphic algorithms for highly accurate and energy-efficient single-snapshot multidimensional harmonic retrieval ({MHR}). By casting the {MHR} problem as a sparse recovery problem, we devise the currently proposed, deep-unrolling-based Structured Learned Iterative Shrinkage and Thresholding (S-{LISTA}) algorithm to solve it efficiently using complex-valued convolutional neural networks with complex-valued activations, which are trained using a supervised regression objective. Afterward, a novel method for converting the complex-valued convolutional layers and activations into spiking neural networks ({SNNs}) is developed. At the heart of this method lies the recently proposed Few Spikes ({FS}) conversion, which is extended by modifying the neuron model's parameters and internal dynamics to account for the inherent coupling between real and imaginary parts in complex-valued computations. Finally, the converted {SNNs} are mapped onto the {SpiNNaker}2 neuromorphic board, and a comparison in terms of estimation accuracy and power efficiency between the original {CNNs} deployed on an {NVIDIA} Jetson Xavier and the {SNNs} is being conducted. The measurement results show that the converted {SNNs} achieve almost five-fold power efficiency at moderate performance loss compared to the original {CNNs}.},
	eventtitle = {2024 58th Asilomar Conference on Signals, Systems, and Computers},
	pages = {298--302},
	booktitle = {2024 58th Asilomar Conference on Signals, Systems, and Computers},
	author = {Andrei, Vlad C. and Drǎgutoiu, Alexandru P. and Béna, Gabriel and Akl, Mahmoud and Li, Yin and Lohrmann, Matthias and Mönich, Ullrich J. and Boche, Holger},
	urldate = {2025-04-30},
	date = {2024-10},
	note = {{ISSN}: 2576-2303},
	keywords = {Accuracy, Convolution, Convolutional neural networks, Harmonic analysis, Iterative algorithms, Loss measurement, Neuromorphics, Neurons, Power measurement, Spiking neural networks},
	file = {andrei_2024_deep- unrolling multidimensional harmonic retrieval algorithms on neuromorphic.pdf:C\:\\Users\\Gabriel\\OneDrive - Imperial College London\\Zotero_All\\Andrei\\andrei_2024_deep- unrolling multidimensional harmonic retrieval algorithms on neuromorphic.pdf:application/pdf},
}

@online{noauthor_spiking_nodate,
	title = {Spiking neural network models of sound localisation via a massively collaborative process {\textbar} {bioRxiv}},
	rights = {All rights reserved},
	url = {https://www.biorxiv.org/content/10.1101/2024.07.19.604252v1.abstract},
	urldate = {2025-04-30},
}

@article{bena_dynamics_2025,
	title = {Dynamics of specialization in neural modules under resource constraints},
	volume = {16},
	rights = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-55188-9},
	doi = {10.1038/s41467-024-55188-9},
	abstract = {The brain is structurally and functionally modular, although recent evidence has raised questions about the extent of both types of modularity. Using a simple, toy artificial neural network setup that allows for precise control, we find that structural modularity does not in general guarantee functional specialization (across multiple measures of specialization). Further, in this setup (1) specialization only emerges when features of the environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar across several different variations of network architectures. Finally, we show that functional specialization varies dynamically across time, and these dynamics depend on both the timing and bandwidth of information flow in the network. We conclude that a static notion of specialization is likely too simple a framework for understanding intelligence in situations of real-world complexity, from biology to brain-inspired neuromorphic systems.},
	pages = {187},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Béna, Gabriel and Goodman, Dan F. M.},
	urldate = {2025-04-30},
	date = {2025-01-02},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Network models},
	file = {béna_2025_dynamics of specialization in neural modules under resource constraints.pdf:C\:\\Users\\Gabriel\\OneDrive - Imperial College London\\Zotero_All\\Béna\\béna_2025_dynamics of specialization in neural modules under resource constraints.pdf:application/pdf},
}


@article{bena_event-based_2025,
  bibtex_show={true},
	title = {Event-based backpropagation on the neuromorphic platform {SpiNNaker}2},
  selected={true},
	url = {http://arxiv.org/abs/2412.15021},
	doi = {10.48550/arXiv.2412.15021},
	abstract = {Neuromorphic computing aims to replicate the brain's capabilities for energy efficient and parallel information processing, promising a solution to the increasing demand for faster and more efficient computational systems. Efficient training of neural networks on neuromorphic hardware requires the development of training algorithms that retain the sparsity of spike-based communication during training. Here, we report on the first implementation of event-based backpropagation on the {SpiNNaker}2 neuromorphic hardware platform. We use {EventProp}, an algorithm for event-based backpropagation in spiking neural networks ({SNNs}), to compute exact gradients using sparse communication of error signals between neurons. Our implementation computes multi-layer networks of leaky integrate-and-fire neurons using discretized versions of the differential equations and their adjoints, and uses event packets to transmit spikes and error signals between network layers. We demonstrate a proof-of-concept of batch-parallelized, on-chip training of {SNNs} using the Yin Yang dataset, and provide an off-chip implementation for efficient prototyping, hyper-parameter search, and hybrid training methods.},
	publisher = {{arXiv}},
	author = {Béna, Gabriel and Wunderlich, Timo and Akl, Mahmoud and Vogginger, Bernhard and Mayr, Christian and Gonzalez, Hector Andres},
	date = {2025-03-19},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Hardware Architecture, Computer Science - Neural and Evolutionary Computing},
}
