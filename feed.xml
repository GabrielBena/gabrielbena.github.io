<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gabrielbena.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gabrielbena.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-13T21:29:16+00:00</updated><id>https://gabrielbena.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">A Path to Universal Neural Cellular Automata</title><link href="https://gabrielbena.github.io/blog/2025/distill/" rel="alternate" type="text/html" title="A Path to Universal Neural Cellular Automata"/><published>2025-05-12T00:00:00+00:00</published><updated>2025-05-12T00:00:00+00:00</updated><id>https://gabrielbena.github.io/blog/2025/distill</id><content type="html" xml:base="https://gabrielbena.github.io/blog/2025/distill/"><![CDATA[<h1 id="a-path-to-universal-neural-cellular-automata">A Path to Universal Neural Cellular Automata</h1> <p>Cellular automata have long been celebrated for their ability to generate complex behaviors from simple, local rules, with well-known discrete models like Conway’s Game of Life proven capable of universal computation.</p> <p>Recent advancements have extended cellular automata into continuous domains, raising the question of whether these systems retain the capacity for universal computation.</p> <p>In parallel, neural cellular automata have emerged as a powerful paradigm where rules are learned via gradient descent rather than manually designed.</p> <p>This work explores the potential of neural cellular automata to develop a continuous Universal Cellular Automaton through training by gradient descent.</p> <p>We introduce a cellular automaton model, objective functions and training strategies to guide neural cellular automata toward universal computation in a continuous setting.</p> <p>Our experiments demonstrate the successful training of fundamental computational primitives — such as matrix multiplication and transposition — culminating in the emulation of a neural network solving the MNIST digit classification task directly within the cellular automata state.</p> <p>These results represent a foundational step toward realizing analog general-purpose computers, with implications for understanding universal computation in continuous dynamics and advancing the automated discovery of complex cellular automata behaviors via machine learning.</p>]]></content><author><name>Gabriel Béna</name></author><category term="neural-cellular-automata"/><category term="universal-computation"/><category term="gradient-descent"/><summary type="html"><![CDATA[Exploring how neural cellular automata can develop continuous universal computation through training by gradient descent]]></summary></entry></feed>